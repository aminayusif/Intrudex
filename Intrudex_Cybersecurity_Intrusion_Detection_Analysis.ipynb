{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNiGvzVJnmPTfnPuoqJBrqw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aminayusif/Intrudex/blob/main/Intrudex_Cybersecurity_Intrusion_Detection_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "5acun7BetrNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook performs an exploratory data analysis and builds machine learning models to detect cybersecurity intrusions. The analysis includes data preprocessing, feature engineering, handling class imbalance, training and evaluating several supervised learning models (Logistic Regression, Random Forest, Decision Tree, and XGBoost), and applying unsupervised learning techniques (K-Means Clustering and Isolation Forest) for anomaly detection. The goal is to identify patterns associated with malicious activities and build models that can effectively predict potential intrusions."
      ],
      "metadata": {
        "id": "fyOX5UpmtUCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Table of Contents\n",
        "\n",
        "- Data Loading and Initial Exploration\n",
        "- Data Preprocessing and Feature Engineering\n",
        "- Handling Class Imbalance\n",
        "- Supervised Learning Models (Training and Evaluation)\n",
        "- Unsupervised Learning (K-Means and Isolation Forest)\n",
        "- Model Interpretation with SHAP\n",
        "- Summary and Conclusion\n",
        "\n",
        "#### Data Loading and Initial Exploration\n",
        "\n",
        "This section covers loading the dataset and performing initial checks such as viewing the head of the dataframe, checking data types and non-null counts, and examining missing values.\n",
        "\n",
        "#### Data Preprocessing and Feature Engineering\n",
        "\n",
        "This section details the steps taken to prepare the data for modeling. This includes:\n",
        "- Dropping irrelevant columns (`session_id`).\n",
        "- Handling missing values in the `encryption_used` column by imputing with the mode.\n",
        "- Creating new features such as the `failed_login_ratio`.\n",
        "- Generating polynomial features (`login_attempts_sq`, `failed_logins_sq`).\n",
        "\n",
        "#### Handling Class Imbalance\n",
        "\n",
        "The notebook addresses the class imbalance in the target variable (`attack_detected`) using the Synthetic Minority Over-sampling Technique (SMOTE). Visualizations are included to show the class distribution before and after applying SMOTE.\n",
        "\n",
        "#### Supervised Learning Models (Training and Evaluation)\n",
        "\n",
        "This section focuses on building and evaluating supervised learning models:\n",
        "- The preprocessed and resampled data is split into training and testing sets.\n",
        "- Logistic Regression, Random Forest, Decision Tree, and XGBoost models are trained and evaluated using GridSearchCV for hyperparameter tuning.\n",
        "- Performance metrics (accuracy, recall, precision, ROC AUC) are calculated and presented in a table, along with classification reports for each model.\n",
        "- ROC AUC curves for all models are plotted for visual comparison.\n",
        "\n",
        "#### Unsupervised Learning (K-Means and Isolation Forest)\n",
        "\n",
        "This section explores unsupervised learning techniques for anomaly detection:\n",
        "- K-Means clustering is applied to identify potential groupings in the data, and the Elbow method is used to suggest an optimal number of clusters.\n",
        "- The clustering results are visualized using scatter plots.\n",
        "- Isolation Forest is applied to detect anomalies, and the distribution of anomaly scores is visualized.\n",
        "\n",
        "#### Model Interpretation with SHAP\n",
        "\n",
        "SHAP (SHapley Additive exPlanations) is used to interpret the Isolation Forest model, providing insights into which features are most influential in determining anomaly scores. A summary plot and a force plot are included to illustrate the SHAP analysis.\n",
        "\n",
        "#### Summary and Conclusion\n",
        "\n",
        "This section summarizes the key findings from both the supervised and unsupervised learning analyses, highlighting the performance of the models and insights gained from the data and model interpretation. Possible next steps are also suggested."
      ],
      "metadata": {
        "id": "2ud4Oojnt2G9"
      }
    }
  ]
}